{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Text Summarization using a Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자연어 처리 테크닉은 활성화되어 있는 추세이다. 그 중 `문서 요약 자동화` 기술은 전체의 컨텐츠의 정보를 보존하면서 사이즈를 줄이는 작업으로써, 굉장히 중요한 과업이다. \n",
    "- 일반적으로, 요약 구조체는 형성하는 것은 굉장히 복잡한 작업으로 깊은 자연어 처리 능력을 요구한다.\n",
    "- 문제를 간단하게 만들기 위해서, 현재 리서치는 추출 생성(extractive-summary generation) 에 집중해오고 있다. 추출 생성은 기존의 텍스트 문장에서 부분을 가져오는 작업을 뜻하는 것으로 이 작업을 통해서 전체 텍스트에서 요약본의 근사치를 편리하게 가져올 수 있다.\n",
    "- 해당 논문에서 우리는 머신 러닝 테크닉을 이용해서 자동화 요약 프로세스를 진행하였다.\n",
    "- 추출 기반 요약 절차의 성공은 휴리스틱(heuristic)을 어떻게 형성할 것인지가 관건이다. 안타깝게도 많은 지표(indicative)만이 성공적인(개연성있는) 특징을 문서 내에서 잡아내었다.\n",
    "- 우리는 해당 논문에서 추출 과정에서 통계적 기법(statistical)과 언어적(linguistic) 기법을 사용했다.\n",
    "- Section 2 는 텍스트 요약 과업에 대한 가벼운 리뷰를 다룬다.\n",
    "- Section 3 에서는 트레이닝이 가능한 요약기의 프레임 워크와 특성(feature)들의 결정 사안에 대해서 자세히 다룬다.\n",
    "- Section 4 에서는 우리는 우리가 적용한 방법에 대한 성능에 대해 이야기한다.\n",
    "- Section 5 에서는 결론에 다다른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 A review of text summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- automatic summarization process 는 3 단계로 나뉜다.\n",
    "    - 전처리 단계(preprocessing step)에서는 구조화된 텍스트로 변환된다.\n",
    "    - 처리 단계(processing step)에서는 텍스트가 요약기를 거쳐 변환된다.\n",
    "    - 생성 단계(generation step)에서는 요약본이 형성된다.\n",
    "- 요약 기법은 언어적 분야에서 두 개로 나뉘어 질 수 있다.\n",
    "    - shallow approahes : \n",
    "        - 통사적 수준에서 유의미한 부분을 추출\n",
    "    - deeper approaches : \n",
    "        - 의미론적 수준에서 유의미한 부분을 추출\n",
    "- 전처리 단계 :\n",
    "    - stop-word elimination : 불용어를 제거해준다.\n",
    "    - case folding : 대 소문자를 일치시켜준다.\n",
    "    - stemming : 통사적으로 유사한 단어로 만들어준다. 단어들의 줄기(stem)을 찾아줌으로써, 의미론적 부분을 강조하기 위함이다.\n",
    "- 자주 쓰이는 텍스트 모델은 벡터 모델(vectorial model)이다. 전처리 단계 이후 각각의 텍스트 요소(문서 요약에서는 문장) 들은 N-차원 벡터로 간주된다.\n",
    "- 이러한 벡터화 과정은 텍스트 요소간의 유사성(similarity)를 계산하게 해준다. 유사성을 측정(measure)하는데 가장 많이 쓰이는 방법은 coine measure로, `cos θ = (<x.y>)` 과 같이 표기한다. 해당 값이 1이면 두 단어의 유사도가 최댓값인 것이고, 0이면 유사도가 없는 것이다.\n",
    "- 생성된 요약본에 대한 질적인 평가는 요약 리서치에 대한 키 포인트이다. TIPSTER Text Summarization Evaluation Conference 에서 요약 테스트 절차에 대한 표준화된 평가 방식을 적용하려 했지만, 평가 기준을 인간이 정하고, 이에 대한 일치성(concordance)에 대한 문제가 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 머신 러닝 접근법은 문서 꾸러미를 가지고 있고, 그에 일치하는 추출된 요약본을 가지는 것이라고 생각할 수 있다.\n",
    "- 우리가 가지고 있는 문서 꾸러미에 기존의 머신 러닝 알고리즘(classical ML algorithm)을 적용하는 것이다.\n",
    "- 이번 경우에는 각각 문서의 문장은 텍스트에서 추출된 특징 벡터로 나타낸다.\n",
    "- 또한, 요약 과업은 correct 와 incorrect 의 클래스가 2개인 분류 문제로 간주된다.\n",
    "    - 요약본이 평가 기준과 일치하면 correct , 그렇지 않으면 incorrect\n",
    "- 머신 러닝을 통해 만들어진 요약기는 해당 문장들의 패턴을 인식하도록 만들어진다.\n",
    "- 요약기 프레임워크의 중요한 이슈는 개연성있고 중요한 특징들을 추출해내는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 A trainable summarizer using a ML approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우리는 두 가지 메인 포인트에 집중해서 프로세스를 나아간다.\n",
    "    1. 선택된 특징들의 세트\n",
    "    2. 요약기로 지정된 프레임워크\n",
    "- 우리들의 특징 세트는 다음과 같이 정의된다.\n",
    "    1. MEAN-TF-ISF : \n",
    "        - 해당 문서 내에서의 단어의 빈도는 positive (term frequency) \n",
    "        - 문서 전체에서의 빈도는 negative (inverse document frequency)\n",
    "        - 두 요소를 곱한다. `term frequency X inverse document frequency`\n",
    "        - 이를 TF-IDF 방법이라 일컫고 이를 문서 요약에서도 적용한다.\n",
    "    2. Sentence Length : \n",
    "        - 너무 짧은 문장에 대해서는 페널티를 부여한다. \n",
    "        - 문장의 길이를 정규화(normalize)한다. \\\n",
    "            - ( 해당 문장의 단어 갯수 / 가장 긴 문장의 단어 갯수)\n",
    "    3. Sentence Position : \n",
    "        - 문장 전체 , 섹션(section) , 단락(paragraph) 별로 포지셔닝을 할 수 있다.\n",
    "        - 해당 논문에서는 전체 문서에서의 percentile 로 정의한다.\n",
    "    4. Similarity to Keywords : \n",
    "         - 전체 문서의 단어들과 해당 문장의 단어들의 similarity를 계산한다.\n",
    "    5. Similarity to Title : \n",
    "        - 해당 문서의 제목과 각 문장들의 similarity 를 계산한다.\n",
    "    6. Sentence-to-Sentence Cohesion : \n",
    "        - 하나의 문장 s가 있을 때, 해당 문서 내의 다른 문장 s_complement 에 대해서 similaruty 를 계산하고 이러한 프로세스에 따라 다른 s 에 대해서도 같은 절차를 실시한다. \n",
    "        - sentence length 에서 실시했던 것과 마찬가지로 정규화 과정을 거친다.\n",
    "    7. Sentence-to-Centriod Cohesion : \n",
    "        - 벡터화된 문장들의 시리즈로 centriod를 구한다.\n",
    "        - centroid , centriod similarity\n",
    "        - centriod , each sentence similarity\n",
    "        - 를 계산함으로써, raw value 를 구하고, normalizing 해준다. (sentence length 와 같은 방식)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 완벽한 수사학적 텍스트 구조에 대한 생성과 분석은 불가능하다는 것이 정설이다.\n",
    "    - 수사학적이란?\n",
    "        - 커뮤니케이션에 단어의 사용에 집중하는 것이다. (ex. 강조 , 은유 , 비유 etc)\n",
    "        - https://en.wikibooks.org/wiki/Rhetoric_and_Composition/Rhetorical_Analysis\n",
    "- 하지만 몇몇 알고리즘이 수사학적 접근법을 기반으로 괜찮은 성능을 보여주고 있다.\n",
    "    - agglomerative clustering algorithm 이 대표적이다.\n",
    "    - 비슷한 문장들은 그들의 어휘적 유사성에 따라(lexical similarity) 함께 그룹화되어 있을 것이다.\n",
    "- 계층적 트리(hierarchical tree)가 형성되면, 이 트리의 루트(root)들은 전체 문서를 대표한다.\n",
    "- 트리는 binary tree이다. 즉, 각각의 스텝별로 두 개의 클러스터로 나뉜다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Depth in the tree : 문장 s에 대한 특징은 트리에서 s에 대한 깊이(depth)이다.\n",
    "2. Referring position in a given level of the tree : 노드가 트리 내에서 위치한 곳에 따라 특징이 정해진다.\n",
    "3. Indicator of main concepts : 메인 컨셉을 나타내는 단어의 형태가 명사라는 관점을 견지하고, 상위 15개의 명사를 뽑아서, 해당 명사가 있으면 True 없으면 False 를 반환한다.\n",
    "4. Occurrence of proper names : 사람의 이름이나 장소를 문장 내에서 포함하면 주요 문장으로 간주하는 지표이다.\n",
    "5. Occurrence of non-essential information : 만약에 문장의 시작점에 'because' , 'furthermore' , 'additionally' 등이 있으면, 부가 설명임을 알 수 있다. 이를 기준점으로 문장 내에 있으면 True , 없으면 False 를 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Review on Automatic Text Summarization Approaches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches to Sentence Extraction \n",
    "- extractive summarization 은 전체 문서에서 중요한 문장을 추출해내는 것이다.\n",
    "- 요약본은 기존의 문장들의 집합체(collections)이다.\n",
    "- 세 가지 접근 방법으로 나뉘어 질 수 있다.\n",
    "    - frequency based approach\n",
    "    - feature based approach\n",
    "    - machine learning based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Based Approach\n",
    "- 단어의 빈도수에 따라서 문장의 중요도를 측정한다.\n",
    "- word probability \n",
    "- term frequency-inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word probability\n",
    "- 빈도수 관점에서 가장 간단한 방법\n",
    "- 문장의 빈도 수 자체를 측정한다.\n",
    "- 문장의 길이에 크게 영향을 받는다.\n",
    "    - 이에 따라 전체 단어의 갯수로 나누어 조정(adjustment)해준다.\n",
    "- $f($w) = n($w) / ($N)\n",
    "    - n(w) : 문장에서 단어 w의 빈도 수\n",
    "    - N : 전체 단어의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency(tf-idf)\n",
    "- 문서에서 자주 등장하는 모든 단어들이 똑같이 중요할까?\n",
    "- tf-idf는 문서들의 집합에서 단어들의 비례적인 빈도를 비교하면서 공통적으로 빈번하게 등장하는 단어들의 가중치를 줄여준다.\n",
    "- tf는 word probability 과 같다.\n",
    "- idf_i = log[|D| / |{d|t_i ∈ d}|]\n",
    "    - 단어 i의 idf는 코퍼스의 총 문서 수를 단어 i가 들어있는 문서 수로 나눈다. \n",
    "- tf - idf = tf_{i,j} * idf_{i,j}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 9,\n",
       " 'is': 3,\n",
       " 'the': 7,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 6,\n",
       " 'and': 0,\n",
       " 'third': 8,\n",
       " 'one': 5,\n",
       " 'last': 4}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = vect.transform(corpus).toarray()\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_word = arr.shape[1]\n",
    "total_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the second second document.'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_probability = [sum(sentence) / total_word for sentence in arr]\n",
    "max_index = word_probability.index(max(word_probability))\n",
    "corpus[max_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf-idf 방법은 기존에 위에서 직접 만들어본, word probability 를 tf(term-frequency)로 둔다.\n",
    "- idf(t) 를 특정한 단어가 들어있는 문서의 수에 반비례하는 수로 넣는데, 공식은 log(n / (1 + df(t)) 이다.\n",
    "    - n : 전체 문서의 수 ex. corpus 의 경우 5\n",
    "    - df(t) : 단어 t 를 가진 문서의 수 ex. This 의 경우, 3 이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf 는 scikit-learn 패키지의 TfidfVenctorizer 로 구현이 되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.24151532, 0.        , 0.28709733, 0.        ,\n",
       "        0.        , 0.85737594, 0.20427211, 0.        , 0.28709733],\n",
       "       [0.55666851, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.55666851, 0.        , 0.26525553, 0.55666851, 0.        ],\n",
       "       [0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.45333103, 0.        , 0.        , 0.80465933,\n",
       "        0.        , 0.        , 0.38342448, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidv = TfidfVectorizer().fit(corpus)\n",
    "arr = tfidv.transform(corpus).toarray()\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 메인 컨셉을 다루고 있는 문장을 추출하고 싶다면, 벡터의 합이 가장 큰, 벡터에 매칭되는 문장을 고르면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the first document.'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_arr = tfidv.transform(corpus).toarray()\n",
    "score_ls = [sum(vec) for vec in tf_idf_arr]\n",
    "max_index = score_ls.index(max(score_ls))\n",
    "corpus[max_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word probability 와는 결과값이 다른 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Based Approach \n",
    "- 문장의 중요도를 파악할 수 있는 가장 자연스러운 방법 중에 하나는 바로, 주요한 문장이 가지는 특징(feature)들을 포착하는 것이다.\n",
    "    - 1. Title/Headline Word :\n",
    "        - 제목에 있는 단어가 문장 내에 발견되면 주요한 문장이라고 간주한다.\n",
    "    - 2. Sentence Position : \n",
    "        - 문서에서 시작점에 위치한 문장은 메인 정보를 다루고 있는 문장이라고 간주한다.\n",
    "    - 3. Sentence Length : \n",
    "        - 짧은 문장은 정보를 담기 부족하며, 긴 문장은 요약을 다루기 적절하지 않다고 간주한다.\n",
    "    - 4. Term weight : \n",
    "        - 문서 내에서 특정 단어가 많이 나오고, 해당 단어를 포함하고 있는 문장은 중요도가 높다고 간주한다.\n",
    "    - 5. Proper Noun : \n",
    "        - 사람 , 조직 , 위치와 같은 명사를 포함하고 있는 문장은 중요도가 높다고 간주한다.\n",
    "<img src=\"fig1 .png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"American Accused of Spying in Russia Is a Marine Veteran, Family Says\"\n",
    "content = \"\"\"\n",
    "LONDON — The American man who was arrested last week in Russia on a spying charge is a Marine Corps veteran who was in Moscow to attend a wedding, his family said on Tuesday.\n",
    "Russia’s Federal Security Service, known as the F.S.B., said on Monday that the American, Paul N. Whelan, had been detained on Friday “during an act of espionage,” and that a criminal case had been opened against him. \n",
    "Conviction on a spying charge in Russia carries a prison sentence of up to 20 years.\n",
    "“We noticed that he was not in communication,” his parents and siblings said in a statement, “which was very much out of character for him even when he was traveling.”\n",
    "They said they had not learned of his arrest until it was reported by the news media on Monday. \n",
    "Since then, they have contacted an array of United States government offices.\n",
    "“We are deeply concerned for his safety and well-being,” the family statement said. “His innocence is undoubted and we trust that his rights will be respected.”\n",
    "Mr. Whelan, 48, worked in corporate security for BorgWarner, an auto parts maker based near Detroit, his twin brother, David, wrote in an email.\n",
    "“We understand that the U.S. government will see him within a 72-hour window that has already begun,” David Whelan wrote.\n",
    "“Paul was attending a wedding for a fellow former Marine,” he wrote. \n",
    "“The wedding party were staying at the Metropol hotel” in Moscow.\n",
    "He added that the family did not know of “anyone who has seen or interacted with him since just before 5 p.m. in Moscow on the 28th.”\n",
    "The arrest of Mr. Whelan came 15 days after a Russian woman, Maria Butina, pleaded guilty in Washington to conspiring to act as a foreign agent, working with Russian officials to influence American political figures. \n",
    "The Kremlin has claimed she is innocent, insisting that Ms. Butina never acted as an agent for Moscow.\n",
    "Tensions between the two capitals have been high over a series of revelations about Russia’s clandestine efforts to influence United States politics and the 2016 election.\n",
    "Russian officials have denied any involvement in such efforts.\n",
    "While it is not yet clear what prompted Mr. Whelan’s arrest, Russia has been known to arrest foreigners with an eye toward trading them for Russians held abroad.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장을 잘라주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LONDON — The American man who was arrested last week in Russia on a spying charge is a Marine Corps veteran who was in Moscow to attend a wedding, his family said on Tuesday.',\n",
       " 'Russia’s Federal Security Service, known as the F.S.B., said on Monday that the American, Paul N. Whelan, had been detained on Friday “during an act of espionage,” and that a criminal case had been opened against him. ',\n",
       " 'Conviction on a spying charge in Russia carries a prison sentence of up to 20 years.']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [i for i in content.split(\"\\n\") if i]\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제목에 있는 단어가 문장에 포함되는지 갯수 세기(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['American',\n",
       " 'Accused',\n",
       " 'of',\n",
       " 'Spying',\n",
       " 'in',\n",
       " 'Russia',\n",
       " 'Is',\n",
       " 'a',\n",
       " 'Marine',\n",
       " 'Veteran,',\n",
       " 'Family',\n",
       " 'Says']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_word = title.split(\" \")\n",
    "title_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_in_sentence = []\n",
    "for sentence in sentences : \n",
    "    count = 0\n",
    "    for word in sentence : \n",
    "        if word in title_word : \n",
    "            count += 1\n",
    "    title_in_sentence.append(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서에서 초반에 등장한 문장일수록, 가중치를 높여주기(Quantile적용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 4)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences) , len(sentences) //4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_score = [1/(i//4 + 1) for i in range(len(sentences)) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장이 너무 짧거나 길지 않은 것에 대해서 가중치 매겨주기\n",
    "- 즉 적절하게 길어야 한다.\n",
    "- 문장의 길이에 따라서 4분위로 나눈다.\n",
    "    - 가장 짧은 문장에 대해서 0.25, \n",
    "    - 가장 긴 문장에 대해서 0.5, \n",
    "    - 2번째로 긴 문장에 대해서 0.75, \n",
    "    - 3번째로 긴 문장에 대해서 1를 매겨준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_ls = [len(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_score = []\n",
    "for sentence in sentences :\n",
    "    if len(sentence) < np.quantile(len_ls,0) : \n",
    "        length_score.append(0.25)\n",
    "    elif len(sentence) < np.quantile(len_ls,0.25) : \n",
    "        length_score.append(0.75)\n",
    "    elif len(sentence) < np.quantile(len_ls,0.5) : \n",
    "        length_score.append(1)\n",
    "    else : length_score.append(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서에서 많이 나오는 단어를 많이 가지고 있는 문장은 가중치를 높게 형성시켜주기\n",
    "- 위에서 다뤘던 tf-idf를 적용해주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidv = TfidfVectorizer().fit(sentences)\n",
    "arr = tfidv.transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_score = [sum(i) for i in arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proper noun 은 simplify 를 위해 생략한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(term_score) == len(length_score) == len(title_in_sentence) == len(loc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score = [term_score[idx] * length_score[idx] * title_in_sentence[idx] * loc_score[idx] for idx in range(len(sentences))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russia’s Federal Security Service, known as the F.S.B., said on Monday that the American, Paul N. Whelan, had been detained on Friday “during an act of espionage,” and that a criminal case had been opened against him. '"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index = total_score.index(max(total_score))\n",
    "sentences[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xplit(*delimiters):\n",
    "    return lambda value: re.split('|'.join([re.escape(delimiter) for delimiter in delimiters]), value)\n",
    "\n",
    "def feature_summary(title,content):\n",
    "    sentences = xplit('. ', '? ', '! ', '\\n', '.\\n')(content)\n",
    "    title_word = title.split(\" \")\n",
    "    title_in_sentence = []\n",
    "    for sentence in sentences : \n",
    "        count = 0\n",
    "        for word in sentence : \n",
    "            if word in title_word : \n",
    "                count += 1\n",
    "        title_in_sentence.append(count)\n",
    "    loc_score = [1/(i//4 + 1) for i in range(len(sentences)) ]\n",
    "    len_ls = [len(sentence) for sentence in sentences]\n",
    "    length_score = []\n",
    "    for sentence in sentences :\n",
    "        if len(sentence) < np.quantile(len_ls,0) : \n",
    "            length_score.append(0.25)\n",
    "        elif len(sentence) < np.quantile(len_ls,0.25) : \n",
    "            length_score.append(0.75)\n",
    "        elif len(sentence) < np.quantile(len_ls,0.5) : \n",
    "            length_score.append(1)\n",
    "        else : length_score.append(0.5)\n",
    "    tfidv = TfidfVectorizer().fit(sentences)\n",
    "    arr = tfidv.transform(sentences).toarray()\n",
    "    term_score = [sum(i) for i in arr]\n",
    "    total_score = [term_score[idx] * length_score[idx] * title_in_sentence[idx] * loc_score[idx] for idx in range(len(sentences))]\n",
    "    max_index = total_score.index(max(total_score))\n",
    "    return sentences[max_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank\n",
    "- 그래프 기반의 문서 요약 기법\n",
    "- PageRank 알고리즘을 기반으로 한다.\n",
    "- 해당 문장에서 나오는 단어들의 similarity 를 기반으로 가중치를 부여한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    " \n",
    "def textrank(document):\n",
    "    sentences = document.split('\\n')\n",
    " \n",
    "    bow_matrix = CountVectorizer().fit_transform(sentences)\n",
    "    normalized = TfidfTransformer().fit_transform(bow_matrix)\n",
    " \n",
    "    similarity_graph = normalized * normalized.T\n",
    " \n",
    "    nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
    "#Creates a new graph from an adjacency matrix given as a SciPy sparse\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    return sorted(((scores[i],s) for i,s in enumerate(sentences)),\n",
    "                  reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"American Accused of Spying in Russia Is a Marine Veteran, Family Says\"\n",
    "content = \"\"\"\n",
    "LONDON — The American man who was arrested last week in Russia on a spying charge is a Marine Corps veteran who was in Moscow to attend a wedding, his family said on Tuesday.\n",
    "Russia’s Federal Security Service, known as the F.S.B., said on Monday that the American, Paul N. Whelan, had been detained on Friday “during an act of espionage,” and that a criminal case had been opened against him. \n",
    "Conviction on a spying charge in Russia carries a prison sentence of up to 20 years.\n",
    "“We noticed that he was not in communication,” his parents and siblings said in a statement, “which was very much out of character for him even when he was traveling.”\n",
    "They said they had not learned of his arrest until it was reported by the news media on Monday. \n",
    "Since then, they have contacted an array of United States government offices.\n",
    "“We are deeply concerned for his safety and well-being,” the family statement said. “His innocence is undoubted and we trust that his rights will be respected.”\n",
    "Mr. Whelan, 48, worked in corporate security for BorgWarner, an auto parts maker based near Detroit, his twin brother, David, wrote in an email.\n",
    "“We understand that the U.S. government will see him within a 72-hour window that has already begun,” David Whelan wrote.\n",
    "“Paul was attending a wedding for a fellow former Marine,” he wrote. \n",
    "“The wedding party were staying at the Metropol hotel” in Moscow.\n",
    "He added that the family did not know of “anyone who has seen or interacted with him since just before 5 p.m. in Moscow on the 28th.”\n",
    "The arrest of Mr. Whelan came 15 days after a Russian woman, Maria Butina, pleaded guilty in Washington to conspiring to act as a foreign agent, working with Russian officials to influence American political figures. \n",
    "The Kremlin has claimed she is innocent, insisting that Ms. Butina never acted as an agent for Moscow.\n",
    "Tensions between the two capitals have been high over a series of revelations about Russia’s clandestine efforts to influence United States politics and the 2016 election.\n",
    "Russian officials have denied any involvement in such efforts.\n",
    "While it is not yet clear what prompted Mr. Whelan’s arrest, Russia has been known to arrest foreigners with an eye toward trading them for Russians held abroad.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LONDON — The American man who was arrested last week in Russia on a spying charge is a Marine Corps veteran who was in Moscow to attend a wedding, his family said on Tuesday.'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked = textrank(content)\n",
    "ranked[0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
